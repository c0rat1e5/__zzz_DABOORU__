#!/usr/bin/env python3
"""
Danbooru Multi-Tag Search & Download (Gradio UI)
- 2ã‚¿ã‚°åˆ¶é™ã‚’å›é¿: APIã«2ã‚¿ã‚°é€ä¿¡ â†’ Pythonå´ã§æ®‹ã‚Šã‚’ãƒ•ã‚£ãƒ«ã‚¿
- Gradio ã§ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã‹ã‚‰ã‚¿ã‚°æ¤œç´¢ãƒ»ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- SDXL ãƒªã‚µã‚¤ã‚º + å¹³å‡è‰²ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
- XMP ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿

ä½¿ã„æ–¹:
  python danbooru_search_ui.py
  â†’ ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:7860 ã‚’é–‹ã
"""
import json
import os
import re
import sys
import time
import shutil
import subprocess
from pathlib import Path
from urllib.parse import urlencode

import requests
import numpy as np
from PIL import Image
import gradio as gr

# ============================================================
# è¨­å®š
# ============================================================
DANBOORU_LOGIN = "palm_floods"
DANBOORU_API_KEY = "Vsq3KWK3pCUGbVPwnDSUtRXF"
PER_PAGE = 200
BASE_URL = "https://danbooru.donmai.us"
AUTH = (DANBOORU_LOGIN, DANBOORU_API_KEY)

SCRIPT_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTPUT_DIR = SCRIPT_DIR / "danbooru_search_results"

# SDXL æ¨™æº–è§£åƒåº¦
SDXL_RESOLUTIONS = [
    (1024, 1024),
    (1152, 896),
    (896, 1152),
    (1216, 832),
    (832, 1216),
    (1344, 768),
    (768, 1344),
    (1536, 640),
    (640, 1536),
]

ALLOWED_EXT = {"jpg", "jpeg", "png", "webp", "gif"}

PREVIEW_PER_PAGE = 50  # ã‚®ãƒ£ãƒ©ãƒªãƒ¼1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®è¡¨ç¤ºæ•°


# ============================================================
# Danbooru API æ¤œç´¢ (2ã‚¿ã‚°åˆ¶é™å›é¿)
# ============================================================
def search_danbooru(
    tags_input: str,
    max_results: int = 100,
    rating_filter: str = "all",
    min_score: int = 0,
    progress_cb=None,
) -> tuple:
    """
    è¤‡æ•°ã‚¿ã‚°æ¤œç´¢:
    - æœ€åˆã®2ã‚¿ã‚°ã‚’ API ã«é€ä¿¡
    - æ®‹ã‚Šã®ã‚¿ã‚°ã¯ Python å´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - é™¤å¤–ã‚¿ã‚° (-tag) ã«ã‚‚å¯¾å¿œ
    """
    tags = tags_input.strip().split()
    if not tags:
        return [], "ã‚¿ã‚°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

    # é™¤å¤–ã‚¿ã‚°ã‚’é™¤ã„ãŸæ¤œç´¢ã‚¿ã‚°ãŒ3ã¤æœªæº€ãªã‚‰æ‹’å¦
    include_only = [t for t in tags if not t.startswith("-")]
    if len(include_only) < 3:
        return [], f"âš ï¸ æ¤œç´¢ã‚¿ã‚°ã‚’3ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆç¾åœ¨ {len(include_only)} å€‹ï¼‰"

    # +ã‚¿ã‚° ã¨ -ã‚¿ã‚° ã‚’åˆ†é›¢
    include_tags = [t for t in tags if not t.startswith("-")]
    exclude_tags = [t.lstrip("-") for t in tags if t.startswith("-")]

    # API ã«é€ã‚‹2ã‚¿ã‚° (æœ€åˆã®2ã¤)
    api_tags = include_tags[:2]
    # Python å´ã§ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹è¿½åŠ ã‚¿ã‚°
    extra_include = include_tags[2:]

    session = requests.Session()
    session.auth = AUTH
    session.headers["User-Agent"] = "DanbooruSearchUI/1.0"

    all_posts = []
    seen_ids = set()
    page = 1
    api_tag_str = " ".join(api_tags)

    status_log = f'APIæ¤œç´¢: "{api_tag_str}"\n'
    if extra_include:
        status_log += f"è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿: {', '.join(extra_include)}\n"
    if exclude_tags:
        status_log += f"é™¤å¤–ã‚¿ã‚°: {', '.join(exclude_tags)}\n"

    # ååˆ†ãªçµæœã‚’å¾—ã‚‹ãŸã‚ã«å¤šã‚ã«å–å¾—
    fetch_limit = max_results * 5 if extra_include else max_results * 2
    fetch_limit = min(fetch_limit, 5000)

    while len(all_posts) < fetch_limit:
        if progress_cb:
            progress_cb(
                len(all_posts) / fetch_limit,
                desc=f"APIå–å¾—ä¸­... {len(all_posts)}/{fetch_limit} posts (page {page})",
            )
        params = {"tags": api_tag_str, "limit": PER_PAGE, "page": page}
        resp = session.get(f"{BASE_URL}/posts.json", params=params)

        if resp.status_code != 200:
            status_log += f"API Error: HTTP {resp.status_code}\n"
            break

        posts = resp.json()
        if not posts:
            break

        for p in posts:
            if p["id"] not in seen_ids:
                seen_ids.add(p["id"])
                all_posts.append(p)

        if len(posts) < PER_PAGE:
            break
        page += 1
        time.sleep(0.3)

    status_log += f"APIå–å¾—: {len(all_posts)} posts\n"

    # --- Python å´ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
    filtered = []
    for p in all_posts:
        post_tags = set(p.get("tag_string", "").split())
        rating = p.get("rating", "")
        score = p.get("score", 0)
        ext = p.get("file_ext", "")

        # ç”»åƒã®ã¿
        if ext.lower() not in ALLOWED_EXT:
            continue

        # è¿½åŠ ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿
        if extra_include and not all(t in post_tags for t in extra_include):
            continue

        # é™¤å¤–ã‚¿ã‚°
        if exclude_tags and any(t in post_tags for t in exclude_tags):
            continue

        # Rating ãƒ•ã‚£ãƒ«ã‚¿
        if rating_filter != "all":
            if rating_filter == "safe" and rating != "g":
                continue
            elif rating_filter == "sensitive" and rating != "s":
                continue
            elif rating_filter == "questionable" and rating != "q":
                continue
            elif rating_filter == "explicit" and rating != "e":
                continue

        # ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿
        if score < min_score:
            continue

        filtered.append(p)

        if len(filtered) >= max_results:
            break

    status_log += f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered)} posts\n"

    return filtered, status_log


# ============================================================
# ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒå–å¾—
# ============================================================
def get_preview_data(posts: list, progress_cb=None) -> list:
    """å„æŠ•ç¨¿ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚®ãƒ£ãƒ©ãƒªãƒ¼ç”¨ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    import tempfile

    preview_dir = Path(tempfile.gettempdir()) / "danbooru_previews"
    preview_dir.mkdir(exist_ok=True)

    session = requests.Session()
    session.headers["User-Agent"] = "DanbooruSearchUI/1.0"

    total = len(posts)
    gallery_items = []
    for idx, p in enumerate(posts):
        if progress_cb and total > 0:
            progress_cb(
                idx / total,
                desc=f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å–å¾—ä¸­... {idx}/{total}",
            )
        pid = p["id"]
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼URL (å°ã•ã„ç”»åƒ)
        preview_url = (
            p.get("preview_file_url")
            or p.get("large_file_url")
            or p.get("file_url", "")
        )
        if not preview_url:
            continue

        # ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        ext = preview_url.rsplit(".", 1)[-1].split("?")[0] or "jpg"
        local_path = preview_dir / f"{pid}.{ext}"
        if not local_path.exists():
            try:
                r = session.get(preview_url, timeout=10)
                if r.status_code == 200:
                    local_path.write_bytes(r.content)
                else:
                    continue
            except:
                continue

        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
        tags_short = p.get("tag_string", "")[:100]
        rating = p.get("rating", "?")
        score = p.get("score", 0)
        caption = f"#{pid} | r:{rating} s:{score} | {tags_short}..."

        gallery_items.append((str(local_path), caption))

    return gallery_items


# ============================================================
# SDXL ãƒªã‚µã‚¤ã‚º
# ============================================================
def find_closest_sdxl_resolution(w: int, h: int) -> tuple:
    aspect = w / h
    best = min(SDXL_RESOLUTIONS, key=lambda r: abs((r[0] / r[1]) - aspect))
    return best


def resize_to_sdxl(filepath: Path) -> bool:
    try:
        img = Image.open(filepath).convert("RGB")
        orig_w, orig_h = img.size
        target_w, target_h = find_closest_sdxl_resolution(orig_w, orig_h)

        if orig_w == target_w and orig_h == target_h:
            return True

        # å¹³å‡è‰²
        arr = np.array(img)
        avg_color = tuple(arr.mean(axis=(0, 1)).astype(int))

        # bicubic ãƒªã‚µã‚¤ã‚º (fit inside)
        scale = min(target_w / orig_w, target_h / orig_h)
        new_w = round(orig_w * scale)
        new_h = round(orig_h * scale)
        img_resized = img.resize((new_w, new_h), Image.BICUBIC)

        # å¹³å‡è‰²ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        canvas = Image.new("RGB", (target_w, target_h), avg_color)
        paste_x = (target_w - new_w) // 2
        paste_y = (target_h - new_h) // 2
        canvas.paste(img_resized, (paste_x, paste_y))

        ext = filepath.suffix.lower()
        if ext in (".jpg", ".jpeg"):
            canvas.save(filepath, "JPEG", quality=95)
        elif ext == ".png":
            canvas.save(filepath, "PNG")
        elif ext == ".webp":
            canvas.save(filepath, "WEBP", quality=95)
        else:
            canvas.save(filepath)
        return True
    except Exception as e:
        print(f"Resize error: {e}")
        return False


# ============================================================
# XMP åŸ‹ã‚è¾¼ã¿
# ============================================================
def embed_xmp(filepath: Path, tags_str: str, rating: str, score: int) -> bool:
    if not shutil.which("exiftool"):
        return False
    full_desc = f"{tags_str} rating:{rating} score:{score}"
    cmd = [
        "exiftool",
        "-overwrite_original",
        f"-XMP:Description={full_desc}",
        f"-XMP:Title={full_desc}",
        "-charset",
        "iptc=UTF8",
    ]
    for tag in tags_str.split():
        cmd.append(f"-XMP:Subject+={tag}")
    cmd += [f"-XMP:Subject+=rating:{rating}", f"-XMP:Subject+=score:{score}"]
    cmd.append(str(filepath))
    try:
        result = subprocess.run(cmd, capture_output=True, text=True)
        return "1 image files updated" in result.stdout
    except:
        return False


# ============================================================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å‡¦ç†
# ============================================================
def download_selected(
    posts_json: str,
    do_resize: bool,
    do_xmp: bool,
    output_folder: str,
    progress=gr.Progress(),
) -> str:
    """é¸æŠã•ã‚ŒãŸæŠ•ç¨¿ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒªã‚µã‚¤ã‚ºãƒ»XMPåŸ‹ã‚è¾¼ã¿"""
    if not posts_json:
        return "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšæ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"

    posts = json.loads(posts_json)
    if not posts:
        return "æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

    out_dir = Path(output_folder) if output_folder else DEFAULT_OUTPUT_DIR
    out_dir.mkdir(parents=True, exist_ok=True)

    session = requests.Session()
    session.headers["User-Agent"] = "DanbooruSearchUI/1.0"

    log = f"å‡ºåŠ›å…ˆ: {out_dir}\n"
    downloaded = 0
    resized = 0
    xmp_count = 0

    for i, p in enumerate(progress.tqdm(posts, desc="Downloading")):
        pid = p["id"]
        ext = p.get("file_ext", "jpg")
        file_url = p.get("file_url") or p.get("large_file_url") or ""
        tags_str = p.get("tag_string", "").replace(" ", ", ")

        if not file_url:
            continue

        fp = out_dir / f"{pid}.{ext}"

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if not fp.exists():
            try:
                r = session.get(file_url, stream=True, timeout=60)
                if r.status_code == 200:
                    with open(fp, "wb") as f:
                        for chunk in r.iter_content(8192):
                            f.write(chunk)
                    downloaded += 1
                else:
                    log += f"#{pid}: HTTP {r.status_code}\n"
                    continue
            except Exception as e:
                log += f"#{pid}: Error {e}\n"
                continue
            time.sleep(0.2)

        # SDXL ãƒªã‚µã‚¤ã‚º
        if do_resize and fp.exists():
            if resize_to_sdxl(fp):
                resized += 1

        # XMP åŸ‹ã‚è¾¼ã¿
        if do_xmp and fp.exists():
            rating = p.get("rating", "")
            score = p.get("score", 0)
            if embed_xmp(fp, tags_str, rating, score):
                xmp_count += 1

    # JSON ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = []
    for p in posts:
        ext = p.get("file_ext", "jpg")
        if ext.lower() not in ALLOWED_EXT:
            continue
        metadata.append(
            {
                "data-id": p["id"],
                "data-tags": p.get("tag_string", "").replace(" ", ", "),
                "data-rating": p.get("rating", ""),
                "data-score": p.get("score", 0),
                "data-uploader-id": p.get("uploader_id", 0),
                "file_url": p.get("file_url", ""),
                "file_ext": ext,
                "source": p.get("source", ""),
                "tag_string_artist": p.get("tag_string_artist", "").replace(" ", ", "),
                "tag_string_character": p.get("tag_string_character", "").replace(
                    " ", ", "
                ),
                "tag_string_copyright": p.get("tag_string_copyright", "").replace(
                    " ", ", "
                ),
            }
        )

    json_path = out_dir / "_search_metadata.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    log += f"\nDone!\n"
    log += f"  Downloaded: {downloaded}\n"
    log += f"  Resized:    {resized}\n"
    log += f"  XMP:        {xmp_count}\n"
    log += f"  JSON:       {json_path.name}\n"
    log += f"  Folder:     {out_dir}\n"
    return log


# ============================================================
# Gradio ã«ã‚ˆã‚‹æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿çŠ¶æ…‹ã®ä¿æŒ
# ============================================================
_current_posts = []


def do_search(
    tags: str, max_results: int, rating: str, min_score: int, progress=gr.Progress()
):
    """æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¿”ã™"""
    global _current_posts

    posts, status = search_danbooru(
        tags, max_results, rating, min_score, progress_cb=progress
    )
    _current_posts = posts
    posts_json = json.dumps(posts)

    # æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã ã‘å–å¾—
    page_posts = posts[:PREVIEW_PER_PAGE]
    gallery_items = get_preview_data(page_posts, progress_cb=progress)

    total_pages = max(1, (len(posts) + PREVIEW_PER_PAGE - 1) // PREVIEW_PER_PAGE)
    page_info = f"ãƒšãƒ¼ã‚¸ 1 / {total_pages}ï¼ˆå…¨ {len(posts)} ä»¶ï¼‰"

    # æ¤œç´¢æ™‚ã¯é¸æŠã‚’ãƒªã‚»ãƒƒãƒˆ
    sel_json = json.dumps([])
    sel_info = f"é¸æŠ: 0 / {len(posts)} ä»¶"

    return gallery_items, status, posts_json, sel_json, sel_info, 0, page_info


def do_page_change(posts_json: str, current_page: int, direction: int, progress=gr.Progress()):
    """ãƒšãƒ¼ã‚¸åˆ‡ã‚Šæ›¿ãˆ"""
    if not posts_json:
        return [], 0, "ãƒ‡ãƒ¼ã‚¿ãªã—"

    posts = json.loads(posts_json)
    total_pages = max(1, (len(posts) + PREVIEW_PER_PAGE - 1) // PREVIEW_PER_PAGE)

    new_page = current_page + direction
    new_page = max(0, min(new_page, total_pages - 1))

    start = new_page * PREVIEW_PER_PAGE
    end = start + PREVIEW_PER_PAGE
    page_posts = posts[start:end]

    gallery_items = get_preview_data(page_posts, progress_cb=progress)
    page_info = f"ãƒšãƒ¼ã‚¸ {new_page + 1} / {total_pages}ï¼ˆå…¨ {len(posts)} ä»¶ï¼‰"

    return gallery_items, new_page, page_info


def do_download(
    posts_json: str,
    selected_json: str,
    do_resize: bool,
    do_xmp: bool,
    output_folder: str,
):
    """é¸æŠã•ã‚ŒãŸæŠ•ç¨¿ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not posts_json:
        return "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšæ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"

    all_posts = json.loads(posts_json)
    selected_indices = set(json.loads(selected_json)) if selected_json else set()

    if not selected_indices:
        return "âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\nã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦é¸æŠ/è§£é™¤ã§ãã¾ã™ã€‚"

    # é¸æŠã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æŠ•ç¨¿ã ã‘æŠ½å‡º
    selected_posts = [
        all_posts[i] for i in sorted(selected_indices) if i < len(all_posts)
    ]
    selected_posts_json = json.dumps(selected_posts)

    return download_selected(selected_posts_json, do_resize, do_xmp, output_folder)


# ============================================================
# Gradio UI
# ============================================================
def create_ui():
    with gr.Blocks(
        title="Danbooru Multi-Tag Search",
    ) as app:
        gr.Markdown("# Danbooru Multi-Tag Search")
        gr.Markdown(
            "2ã‚¿ã‚°åˆ¶é™ã‚’å›é¿ï¼ è¤‡æ•°ã‚¿ã‚°ã§æ¤œç´¢ â†’ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n\n"
            "**é™¤å¤–ã‚¿ã‚°**: `-tag` ã§é™¤å¤– (ä¾‹: `1girl blue_hair -comic`)"
        )

        posts_state = gr.State("")
        selected_state = gr.State("[]")
        page_state = gr.State(0)  # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ (0-indexed)

        with gr.Row():
            with gr.Column(scale=3):
                tags_input = gr.Textbox(
                    label="ã‚¿ã‚° (ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šãƒ»3å€‹ä»¥ä¸Šå¿…é ˆ)",
                    placeholder="1girl blue_hair large_breasts highres solo -comic -monochrome",
                    lines=2,
                )
                tag_warning = gr.Markdown(
                    value="âš ï¸ **æ¤œç´¢ã‚¿ã‚°ã‚’3ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„**ï¼ˆé™¤å¤–ã‚¿ã‚° `-tag` ã¯ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã›ã‚“ï¼‰",
                    visible=True,
                )
            with gr.Column(scale=1):
                max_results = gr.Slider(
                    minimum=10, maximum=1000, value=200, step=10, label="æœ€å¤§ä»¶æ•°"
                )
                min_score = gr.Number(value=0, label="æœ€ä½ã‚¹ã‚³ã‚¢", precision=0)

        with gr.Row():
            rating_filter = gr.Radio(
                choices=["all", "safe", "sensitive", "questionable", "explicit"],
                value="all",
                label="Rating ãƒ•ã‚£ãƒ«ã‚¿",
            )
            search_btn = gr.Button(
                "ğŸ” æ¤œç´¢", variant="primary", size="lg", interactive=False
            )

        status_text = gr.Textbox(label="æ¤œç´¢ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", interactive=False, lines=4)

        gallery = gr.Gallery(
            label="æ¤œç´¢çµæœï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é¸æŠ/è§£é™¤ï¼‰",
            columns=5,
            rows=4,
            height="auto",
            object_fit="contain",
        )

        # --- ãƒšãƒ¼ã‚¸ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ---
        with gr.Row():
            prev_page_btn = gr.Button("â—€ å‰ãƒšãƒ¼ã‚¸", size="sm")
            page_info = gr.Markdown(value="ãƒšãƒ¼ã‚¸ 0 / 0")
            next_page_btn = gr.Button("æ¬¡ãƒšãƒ¼ã‚¸ â–¶", size="sm")

        # --- é¸æŠæ“ä½œ UI ---
        with gr.Row():
            select_all_btn = gr.Button("âœ… å…¨é¸æŠ", size="sm")
            select_page_btn = gr.Button("â˜‘ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠ", size="sm")
            deselect_all_btn = gr.Button("âŒ å…¨è§£é™¤", size="sm")
            selected_info = gr.Markdown(value="é¸æŠ: 0 / 0 ä»¶")

        gr.Markdown("---")
        gr.Markdown("### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®š")

        with gr.Row():
            do_resize = gr.Checkbox(
                value=True, label="SDXL ãƒªã‚µã‚¤ã‚º (å¹³å‡è‰²ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°)"
            )
            do_xmp = gr.Checkbox(value=True, label="XMP ã‚¿ã‚°åŸ‹ã‚è¾¼ã¿ (exiftool)")

        output_folder = gr.Textbox(
            label="ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€",
            value=str(DEFAULT_OUTPUT_DIR),
        )

        with gr.Row():
            download_btn = gr.Button(
                "â¬‡ï¸ é¸æŠç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", variant="primary", size="lg"
            )

        download_log = gr.Textbox(label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ­ã‚°", interactive=False, lines=8)

        # --- ã‚¿ã‚°æ•°ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ---
        def validate_tags(text):
            include = [t for t in text.strip().split() if not t.startswith("-")]
            count = len(include)
            if count >= 3:
                return (
                    gr.update(interactive=True),
                    gr.update(
                        value=f"âœ… **æ¤œç´¢ã‚¿ã‚°: {count} å€‹** â€” æ¤œç´¢ã§ãã¾ã™",
                        visible=True,
                    ),
                )
            else:
                return (
                    gr.update(interactive=False),
                    gr.update(
                        value=f"âš ï¸ **æ¤œç´¢ã‚¿ã‚°ã‚’3ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„**ï¼ˆç¾åœ¨ {count} å€‹ã€é™¤å¤–ã‚¿ã‚° `-tag` ã¯ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã›ã‚“ï¼‰",
                        visible=True,
                    ),
                )

        tags_input.change(
            fn=validate_tags,
            inputs=[tags_input],
            outputs=[search_btn, tag_warning],
        )

        # --- ã‚®ãƒ£ãƒ©ãƒªãƒ¼é¸æŠãƒãƒ³ãƒ‰ãƒ© ---
        def on_gallery_select(selected_json, posts_json, current_page, evt: gr.SelectData):
            """ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸã¨ãã€é¸æŠã‚’ãƒˆã‚°ãƒ« (ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ç®¡ç†)"""
            selected = set(json.loads(selected_json)) if selected_json else set()
            # ã‚®ãƒ£ãƒ©ãƒªãƒ¼ä¸Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ â†’ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            global_idx = current_page * PREVIEW_PER_PAGE + evt.index
            if global_idx in selected:
                selected.discard(global_idx)
            else:
                selected.add(global_idx)

            total = len(json.loads(posts_json)) if posts_json else 0
            sel_json = json.dumps(sorted(selected))
            info = f"**é¸æŠ: {len(selected)} / {total} ä»¶**"
            if len(selected) > 0:
                info += " â€” ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½"
            return sel_json, info

        def select_all(posts_json):
            """å…¨é¸æŠ"""
            posts = json.loads(posts_json) if posts_json else []
            all_indices = list(range(len(posts)))
            return (
                json.dumps(all_indices),
                f"**é¸æŠ: {len(posts)} / {len(posts)} ä»¶** â€” ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½",
            )

        def select_current_page(selected_json, posts_json, current_page):
            """ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’ã™ã¹ã¦é¸æŠã«è¿½åŠ """
            selected = set(json.loads(selected_json)) if selected_json else set()
            posts = json.loads(posts_json) if posts_json else []
            start = current_page * PREVIEW_PER_PAGE
            end = min(start + PREVIEW_PER_PAGE, len(posts))
            for i in range(start, end):
                selected.add(i)
            sel_json = json.dumps(sorted(selected))
            info = f"**é¸æŠ: {len(selected)} / {len(posts)} ä»¶**"
            if len(selected) > 0:
                info += " â€” ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½"
            return sel_json, info

        def deselect_all(posts_json):
            """å…¨è§£é™¤"""
            total = len(json.loads(posts_json)) if posts_json else 0
            return json.dumps([]), f"é¸æŠ: 0 / {total} ä»¶"

        gallery.select(
            fn=on_gallery_select,
            inputs=[selected_state, posts_state, page_state],
            outputs=[selected_state, selected_info],
        )

        select_all_btn.click(
            fn=select_all,
            inputs=[posts_state],
            outputs=[selected_state, selected_info],
        )

        select_page_btn.click(
            fn=select_current_page,
            inputs=[selected_state, posts_state, page_state],
            outputs=[selected_state, selected_info],
        )

        deselect_all_btn.click(
            fn=deselect_all,
            inputs=[posts_state],
            outputs=[selected_state, selected_info],
        )

        # --- ãƒšãƒ¼ã‚¸ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ ---
        prev_page_btn.click(
            fn=lambda pj, cp: do_page_change(pj, cp, -1),
            inputs=[posts_state, page_state],
            outputs=[gallery, page_state, page_info],
        )

        next_page_btn.click(
            fn=lambda pj, cp: do_page_change(pj, cp, +1),
            inputs=[posts_state, page_state],
            outputs=[gallery, page_state, page_info],
        )

        # ã‚¤ãƒ™ãƒ³ãƒˆæ¥ç¶š
        search_btn.click(
            fn=do_search,
            inputs=[tags_input, max_results, rating_filter, min_score],
            outputs=[
                gallery, status_text, posts_state,
                selected_state, selected_info,
                page_state, page_info,
            ],
        )

        # Enter ã‚­ãƒ¼ã§ã‚‚æ¤œç´¢
        tags_input.submit(
            fn=do_search,
            inputs=[tags_input, max_results, rating_filter, min_score],
            outputs=[
                gallery, status_text, posts_state,
                selected_state, selected_info,
                page_state, page_info,
            ],
        )

        download_btn.click(
            fn=do_download,
            inputs=[posts_state, selected_state, do_resize, do_xmp, output_folder],
            outputs=[download_log],
        )

    return app


# ============================================================
# ãƒ¡ã‚¤ãƒ³
# ============================================================
if __name__ == "__main__":
    app = create_ui()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True,
    )
